{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costly conversion data challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2020-02-24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leslie Emery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem:\n",
    "- Goal is to evaluate a pricing A/B test\n",
    "- Focus on user segmentation and provide insights about user segments that behave differently\n",
    "- Condition A: old price $39 (66%)\n",
    "- Condition B: new price $59 (33%)\n",
    "- Does it make sense to increase the price?\n",
    "\n",
    "### Questions to answer:\n",
    "- Should the price be $39 or $59?\n",
    "- What are my main findings looking at the data?\n",
    "- How long should the test have been run to find significant results?\n",
    "\n",
    "### My approach:\n",
    "- Overall data exploration\n",
    "    - Plot all of the data\n",
    "    - Segment users (by source, by device, by OS, by user age)\n",
    "       - Could segment by day of week or time of day!\n",
    "    - Consider the customer funnel\n",
    "       - Don't really have data for this, or for geographic segmentation\n",
    "- Statistical test for A/B difference (chisq?)\n",
    "- Cost analysis of the results\n",
    "- Power analysis for determining how long to run test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- My key metric is conversion rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My conclusions:\n",
    "- \n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://www.priceintelligently.com/blog/bid/180676/why-you-should-never-a-b-test-your-pricing-strategy\n",
    "- https://envisionitagency.com/blog/2015/02/pricing-optimization-with-ab-and-multivariate-testing/\n",
    "- https://help.optimizely.com/Analyze_Results/How_long_to_run_an_experiment#baseline\n",
    "- https://www.invespcro.com/blog/calculating-sample-size-for-an-ab-test/\n",
    "- https://datascience.stackexchange.com/questions/11469/how-would-i-chi-squared-test-these-simple-results-from-a-b-experiment\n",
    "- https://www.mikulskibartosz.name/how-to-perform-an-ab-test-correctly-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# from pygeocoder import Geocoder\n",
    "from scipy.stats import chi2_contingency, ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '~/devel/insight-data-challenges/03-costly-conversion/data/Pricing_Test_data'\n",
    "# Read in and clean the tests data\n",
    "tests = pd.read_csv(\n",
    "    os.path.join(os.path.expanduser(data_dir), 'test_results.csv'),\n",
    "    parse_dates=['timestamp']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.head()\n",
    "tests.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out why timestamp column can't be parsed to datetime\n",
    "tests['timestamp'].head(20)\n",
    "tests['timestamp'].describe()\n",
    "dt = pd.to_datetime(tests['timestamp'], errors='coerce')\n",
    "dt.describe()\n",
    "tests.loc[dt.isna()].head(50)\n",
    "tests.loc[dt.isna()].tail(50)\n",
    "# How many are affected?\n",
    "sum(dt.isna())\n",
    "# 10271\n",
    "sum(dt.isna()) / tests.shape[0]\n",
    "# ~3%\n",
    "# This fails\n",
    "# datetime.strptime(tests['timestamp'].iloc[1053], \"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the problematic strings, replace all \"60\" in minutes or seconds with \"59\"\n",
    "minutes_60 = re.compile(r'(?P<hour>\\d+):(?P<minute>60):(?P<second>\\d{2})')\n",
    "minutes_replace = r'\\g<hour>:59:\\g<second>'\n",
    "seconds_60 = re.compile(r'(?P<hour>\\d+):(?P<minute>\\d{2}):(?P<second>60)')\n",
    "seconds_replace = r'\\g<hour>:\\g<minute>:59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests['timestamp'] = pd.to_datetime(tests['timestamp'].str.replace(\n",
    "    minutes_60, minutes_replace).str.replace(\n",
    "    seconds_60, seconds_replace), errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.head()\n",
    "tests.info()\n",
    "tests.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "category_columns = [v for v in tests.columns if tests[v].nunique() < 20]\n",
    "for v in category_columns:\n",
    "    print(tests[v].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in and clean the users data\n",
    "users = pd.read_csv(\n",
    "    os.path.join(os.path.expanduser(data_dir), 'user_table.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head()\n",
    "users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = [v for v in users.columns if users[v].nunique() < 100]\n",
    "for v in category_columns:\n",
    "    print(users[v].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All users are from the USA and there isn't the data to determine user age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the US state name based on lat/long - DOES NOT WORK\n",
    "def get_state(row):\n",
    "    return Geocoder.reverse_geocode(row['lat'], row['long']).administrative_area_level_1\n",
    "\n",
    "\n",
    "users_row = users.iloc[0]\n",
    "Geocoder.reverse_geocode(users_row['lat'], users_row['long']).administrative_area_level_1\n",
    "users['us_state'] = users[['lat', 'long']].apply(get_state, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"data-challenge\")\n",
    "users_row = users.iloc[0]\n",
    "location = geolocator.reverse('{}, {}'.format(users_row['lat'], users_row['long']))\n",
    "dir(location)\n",
    "print(location.address)\n",
    "location.raw['address']['postcode']\n",
    "location.raw['address']['state']\n",
    "\n",
    "def get_geo_data(row):\n",
    "    location = geolocator.reverse('{}, {}'.format(row['lat'], row['long']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works but has a max rate limit of 1 query per second, which would take about 76 hours to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From https://public.opendatasoft.com/explore/dataset/us-zip-code-latitude-and-longitude/table/\n",
    "cities = pd.read_csv(\n",
    "    os.path.join(os.path.expanduser(data_dir), 'us-zip-code-latitude-and-longitude.csv'),\n",
    "    sep=';'\n",
    ")\n",
    "\n",
    "cities.info()\n",
    "cities.head()\n",
    "\n",
    "cities['latitude_round'] = cities['Latitude'].round(2)\n",
    "cities['longitude_round'] = cities['Longitude'].round(2)\n",
    "\n",
    "users_row\n",
    "cities.loc[(cities['latitude_round'] == users_row['lat']) & (cities['longitude_round'] == users_row['long'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also not very fruitful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the overall conversion rate for each group?\n",
    "conversions = tests.groupby('price').aggregate(\n",
    "    conversion_rate=('converted', lambda x: sum(x) / len(x)),\n",
    "    conversion_count=('converted', 'sum'),\n",
    "    nonconversion_count=('converted', lambda x: len(x) - sum(x)),\n",
    "    visitor_count=('user_id', 'count')\n",
    ")\n",
    "conversions = conversions.reset_index()\n",
    "conversions['revenue_per_visitor'] = conversions['conversion_count'] * conversions['price'] / conversions['visitor_count']\n",
    "print(conversions)\n",
    "# Even with the decrease in conversion rate, the revenue earned per visitor is up by $0.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the difference in conversion rate significant?\n",
    "chi2, pvalue, dof, ex = chi2_contingency(conversions[['conversion_count', 'nonconversion_count']].transpose())\n",
    "print('The decreased conversion rate of {:.3f} is statististically significant with p={:.3f}'.format(\n",
    "    conversions['conversion_rate'].diff().max(),\n",
    "    pvalue\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the difference in revenue significant?\n",
    "this doesn't seem like a valid question to ask here because it's just another version of \"are these two numbers different?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all of the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots of each variable by conversion\n",
    "tests_melted = tests.melt(id_vars=['user_id', 'timestamp', 'converted', 'test', 'price'])\n",
    "tests_melted_conversion_rate = tests_melted.groupby(['variable', 'value', 'test']).agg(\n",
    "    conversion_rate=('converted', lambda x: sum(x) / len(x)),\n",
    "    converted_count=('converted', lambda x: sum(x)),\n",
    "    unconverted_count=('converted', lambda x: len(x) - sum(x))\n",
    ")\n",
    "tests_melted_conversion_rate = tests_melted_conversion_rate.reset_index()\n",
    "tests_melted_conversion_rate['test_condition'] = tests_melted_conversion_rate['test'].replace({0: 'Old price (A)',\n",
    "                                                                                               1: 'New price (B)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 'device'\n",
    "for v in tests_melted_conversion_rate['variable'].unique():\n",
    "    fig = px.bar(\n",
    "        tests_melted_conversion_rate.loc[tests_melted_conversion_rate['variable'] == v].sort_values(\n",
    "            'conversion_rate', ascending=False),\n",
    "        x='value', y='conversion_rate', color='test_condition',\n",
    "        title=\"Conversion rate by {}\".format(v.title()),\n",
    "        template='plotly_white',\n",
    "        barmode='group'\n",
    "    )\n",
    "    fig.update_xaxes(matches=None)\n",
    "    fig.update_traces(marker_line_width=3)\n",
    "    fig.update_layout(bargap=0.1, xaxis_title=v.title())\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in tests_melted_conversion_rate['variable'].unique():\n",
    "    fig = px.bar(\n",
    "        tests_melted_conversion_rate.loc[tests_melted_conversion_rate['variable'] == v].sort_values(\n",
    "            'converted_count', ascending=False),\n",
    "        x='value', y='converted_count', color='test_condition',\n",
    "        title=\"Conversion count by {}\".format(v.title()),\n",
    "        template='plotly_white',\n",
    "        barmode='group'\n",
    "    )\n",
    "    fig.update_xaxes(matches=None)\n",
    "    fig.update_traces(marker_line_width=3)\n",
    "    fig.update_layout(bargap=0.1, xaxis_title=v.title())\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What time were these sales happening over?\n",
    "tests.groupby(['test', 'converted']).agg(Max=('timestamp', 'max'), Min=('timestamp', 'min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot conversions over time\n",
    "tests_sorted = tests.copy()\n",
    "tests_sorted = tests_sorted.sort_values('timestamp', ascending=True)\n",
    "tests_1_cumulative_conversions = tests_sorted.loc[tests_sorted['test'] == 1]\n",
    "tests_1_cumulative_conversions['cumulative_conversions'] = tests_1_cumulative_conversions['converted'].cumsum()\n",
    "tests_0_cumulative_conversions = tests_sorted.loc[tests_sorted['test'] == 0]\n",
    "tests_0_cumulative_conversions['cumulative_conversions'] = tests_0_cumulative_conversions['converted'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_cumulative_conversions = tests_1_cumulative_conversions[['timestamp', 'cumulative_conversions', 'test']].append(\n",
    "    tests_0_cumulative_conversions[['timestamp', 'cumulative_conversions', 'test']])\n",
    "tidy_cumulative_conversions['test_condition'] = tidy_cumulative_conversions['test'].replace({0: 'Old price (A)',\n",
    "                                                                                             1: 'New price (B)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    tidy_cumulative_conversions,\n",
    "    x='timestamp', y='cumulative_conversions', color='test_condition',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Power calculations\n",
    "sample_conversion_rate = tests.loc[tests['test'] == 0, 'converted'].sum() / tests.loc[tests['test'] == 0, 'converted'].shape[0]\n",
    "alpha = 0.05\n",
    "mde = 0.2\n",
    "# Also called \"desired lift\"\n",
    "power_target = 0.9  # min improvement for B condition to be worthwhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.power import GofChisquarePower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "analysis = GofChisquarePower()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
