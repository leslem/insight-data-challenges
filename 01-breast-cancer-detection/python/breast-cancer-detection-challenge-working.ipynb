{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast cancer detection data challenge\n",
    "2020-02-08\n",
    "See the pdf in the GDrive, which I can't link to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "You belong to the data team at a local research hospital. You've been tasked with developing a means to help doctors\n",
    "diagnose breast cancer. You've been given data about biopsied breast cells; where it is benign (not harmful) or\n",
    "malignant (cancerous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What features of a cell are the largest drivers of malignancy?\n",
    "2. How would a physician use your product?\n",
    "3. There is a non-zero cost in time and money to collect each feature about a given cell.\n",
    "How would you go about determining the most cost-effective method of detecting malignancy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and take a look at it\n",
    "data_dir = '~/devel/insight-data-challenges/01-breast-cancer-detection/data'\n",
    "cancer_df = pd.read_csv(\n",
    "    os.path.join(os.path.expanduser(data_dir), 'breast-cancer-wisconsin.txt'),\n",
    "    index_col='Index',\n",
    ")\n",
    "# These are all read in as str (object) dtypes because they have string missing codes\n",
    "print(cancer_df.head())\n",
    "print(cancer_df.info())\n",
    "print(cancer_df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Are there duplicates per ID value?\n",
    "print(cancer_df['ID'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, there are 666 unique IDs and almost 16,000 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove exact duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = cancer_df[~cancer_df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data values for every column\n",
    "for v in cancer_df.columns:\n",
    "    print(cancer_df[v].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many missing value codes to account for\n",
    "?, No idea, #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace non-standard missing value codes with NaN\n",
    "cancer_df = cancer_df.replace(['?', 'No idea', '#'], np.NaN)\n",
    "\n",
    "# Drop rows with any missing data\n",
    "old_nrows = cancer_df.shape[0]\n",
    "cancer_df = cancer_df.dropna(how='any')\n",
    "new_nrows = cancer_df.shape[0]\n",
    "print('{} rows with missing data removed'.format(old_nrows - new_nrows))\n",
    "\n",
    "# Replace outcome codes with meaningful labels\n",
    "cancer_df['Class'] = cancer_df['Class'].replace({'2': 0, '4': 1})\n",
    "print(cancer_df['Class'].value_counts())\n",
    "# It's easiest for the use of LogisticRegression later on to use 0/1 coding\n",
    "# There are some extra values here - 40 and 20, which might be typos for 2 and 4, or might be valid data?\n",
    "\n",
    "# Now fix the data types\n",
    "print(cancer_df.info())\n",
    "str_columns = cancer_df.select_dtypes(include='object').columns.to_list()\n",
    "str_columns = [c for c in str_columns if c != 'Class']\n",
    "cancer_df[str_columns] = cancer_df[str_columns].astype('int32')\n",
    "print(cancer_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a \"tidy\" version of the data\n",
    "cancer_df_tidy = cancer_df.melt(id_vars=['ID', 'Class'])\n",
    "print(cancer_df_tidy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all of the variables vs. the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(cancer_df_tidy, x='variable', y='value', color='Class', facet_col='variable', facet_col_wrap=3)\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.update_xaxes(matches=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples with Class = 20 or 40 all have outlier values for the other variables, outside of the valid ranges specified\n",
    "in the pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop these values because I can't be sure that dividing them by ten will give me valid data.\n",
    "Make the conservative choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_shape = cancer_df.shape\n",
    "outlier_samples = cancer_df['Class'].isin(['20', '40'])\n",
    "cancer_df = cancer_df.loc[~outlier_samples]\n",
    "new_shape = cancer_df.shape\n",
    "print(cancer_df.head())\n",
    "print('{} rows removed for outlier values'.format(old_shape[0] - new_shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remake the tidy dataset\n",
    "cancer_df_tidy = cancer_df.melt(id_vars=['ID', 'Class'])\n",
    "print(cancer_df_tidy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all of the variables vs. the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "fig = px.box(cancer_df_tidy, x='variable', y='value', color='Class', facet_col='variable', facet_col_wrap=3)\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.update_xaxes(matches=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some very drastic differences here between the two Class values, so logistic regression should do well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the features are already in the same range (0 - 10), so I don't need to worry about feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(\n",
    "    cancer_df,\n",
    "    dimensions=cancer_df.columns.difference(['Class', 'ID']),\n",
    "    color='Class', symbol='Class',\n",
    "    title=\"Scatter matrix of breast cancer data set\"\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot technically works, but doesn't look good because of the discrete values.\n",
    "A scatter plot is not really appropriate here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and training split\n",
    "X = cancer_df[cancer_df.columns.difference(['Class', 'ID'])]\n",
    "y = cancer_df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model on the entire dataset\n",
    "classifier = LogisticRegression(random_state=48).fit(X, y)\n",
    "dir(classifier)\n",
    "\n",
    "fig = px.bar(x=classifier.coef_[0], y=X.columns, title='Coeffcients', orientation='h')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning by grid search\n",
    "Logistic regression can be tuned over the parameter C, which controls whether the model tries to keep coefficients\n",
    "close to 0 or not. You can also tune on L1 vs. L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hyperparameter options\n",
    "# All of the hyperparameters are related to regularization, which is a way to penalize non-intercept coefficients\n",
    "# as a way to prevent overfitting. There is a penalty for any non-zero coefficient for the features.\n",
    "param_grid = {\n",
    "    # C = inverse of regularization strength; positive float; smaller C = stronger regularization\n",
    "    'C': np.logspace(-5, 5, 11),  # Similar to defaults for LogisticRegressionCV\n",
    "    'penalty': ['l1', 'l2']  # Regularization penalty type; L1 = Lasso, L2 = Ridge\n",
    "    # L1 = penalized by absolute value of coefficient magnitude\n",
    "    # L2 = penalized by squared magnitude of coefficient\n",
    "}\n",
    "\n",
    "# Do a grid search over the parameter values tried, using 5-fold CV.\n",
    "# Then evaluate the scores for each set of parameter values.\n",
    "# Use liblinear solver because it supports both L1 and L2 penalty, and works well on smaller data.\n",
    "classifier_grid_search = GridSearchCV(\n",
    "    LogisticRegression(solver='liblinear', random_state=48),\n",
    "    param_grid, cv=5, scoring='f1', verbose=0\n",
    ")\n",
    "grid_search_models = classifier_grid_search.fit(X, y)\n",
    "\n",
    "grid_search_results = pd.DataFrame(grid_search_models.cv_results_)\n",
    "grid_search_results['params_string'] = grid_search_results['params'].apply(\n",
    "    lambda x: 'C={:.3f}<br>Penalty={}'.format(x['C'], x['penalty']))\n",
    "grid_search_results['mean_test_score']\n",
    "\n",
    "fig = px.bar(grid_search_results.sort_values(by='rank_test_score', ascending=True),\n",
    "             x=grid_search_results.index.to_list(), y='mean_test_score')\n",
    "fig.update_layout(\n",
    "    yaxis={'range': (0, 1)},\n",
    "    xaxis={\n",
    "        'tickmode': 'array',\n",
    "        'tickvals': grid_search_results.index.to_list(),\n",
    "        'ticktext': grid_search_results['params_string']\n",
    "    }\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier = grid_search_models.best_estimator_\n",
    "best_classifier.get_params()\n",
    "\n",
    "# coef_ gives the coefficients contributing to classes_[1], which is \"malignant\"\n",
    "best_classifier.classes_\n",
    "\n",
    "coefficients_df = pd.DataFrame(data=best_classifier.coef_,\n",
    "                               columns=X.columns)\n",
    "coefficients_df = coefficients_df.melt()\n",
    "coefficients_df['absolute_value'] = coefficients_df['value'].abs()\n",
    "coefficients_df['direction'] = coefficients_df['value'].apply(np.sign).replace({-1: 'negative', 1: 'positive'})\n",
    "coefficients_df = coefficients_df.sort_values(by='absolute_value', ascending=True)\n",
    "fig = px.bar(\n",
    "    coefficients_df, x='absolute_value', y='variable', color='direction', orientation='h',\n",
    "    title='Drivers of malignancy',\n",
    "    labels={'absolute_value': 'Coefficient magnitude', 'direction': 'Coefficient direction', 'variable': ''}\n",
    ")\n",
    "fig.update_layout(yaxis={'categoryorder': 'total ascending'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to always classifying as the dominant class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# The data is not so imbalanced since I removed the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df['Class'].value_counts()\n",
    "\n",
    "dummy_majority_classifier = DummyClassifier(strategy='most_frequent')\n",
    "dummy_majority_fit = dummy_majority_classifier.fit(X_train, y_train)\n",
    "print('Just classifying everything as malignant gets you {:.3f} accuracy'.format(\n",
    "    dummy_majority_fit.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix on best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, best_classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision = proportion of positive calls that are correct; good for optimizing on low FP rate\n",
    "- Recall = proportion of all ground truth positives that are called correctly; good for optimizing on low FN rate\n",
    "- F-score = harmonic mean of precision and recall (F1-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score should be the best for optimizing precision vs. recall in this imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, best_classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a fake patient by randomly selecting a value from each feature\n",
    "fake_patient = X.apply(np.random.choice, axis=0)\n",
    "fake_prediction = best_classifier.predict(np.array([fake_patient.to_numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to account for data collection cost for each feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try recursive feature elmination with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv = RFECV(\n",
    "    estimator=LogisticRegression(solver='liblinear',\n",
    "                                 C=best_classifier.get_params()['C'],\n",
    "                                 penalty=best_classifier.get_params()['penalty'],\n",
    "                                 random_state=48),\n",
    "    step=1, cv=5, scoring='f1')\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print('F1 scores for each feature:')\n",
    "print(['{:.3f}'.format(x) for x in rfecv.grid_scores_])\n",
    "\n",
    "print('Recommended to select {} of the following features:'.format(rfecv.min_features_to_select))\n",
    "print(X.columns[rfecv.support_])\n",
    "\n",
    "print('All features except for Uniformity of Cell Size are tied because F1 is high for every one:')\n",
    "print(pd.DataFrame({'rank': rfecv.ranking_, 'feature': X.columns.to_list()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any individual feature will get comparable accuracy to a model with all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_classifier = LogisticRegressionCV(solver='liblinear',\n",
    "                                     Cs=[best_classifier.get_params()['C']],\n",
    "                                     penalty=best_classifier.get_params()['penalty'],\n",
    "                                     cv=5, random_state=48, scoring='f1')\n",
    "\n",
    "single_feature_scores = []\n",
    "for v in X.columns:\n",
    "    feature = X[[v]]\n",
    "    single_feature_model = cv_classifier.fit(feature, y)\n",
    "    single_feature_scores.append(np.mean(single_feature_model.scores_[1]))\n",
    "\n",
    "single_feature_scores = pd.DataFrame({'feature': X.columns.to_list(), 'score': single_feature_scores})\n",
    "single_feature_scores = single_feature_scores.append({'feature': 'All features', 'score': grid_search_models.best_score_},\n",
    "                                                     ignore_index=True)\n",
    "single_feature_scores = single_feature_scores.sort_values(by='score', ascending=True)\n",
    "fig = px.bar(single_feature_scores, x='feature', y='score',\n",
    "             title='5-fold CV F1 scores for single-feature models compared to a model with all features',\n",
    "             labels={'feature': 'Feature', 'score': 'F1'})\n",
    "# fig.update_layout(yaxis={'range': (0.9, 1.0)})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For executing all code from interpreter:\n",
    "# with open('breast-cancer-detection-challenge-working.py', 'r') as f:\n",
    "#     exec(f.read())"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
